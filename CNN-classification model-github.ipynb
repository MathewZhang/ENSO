{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import copy\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, precision_score, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "# 绘制混淆矩阵图\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_class, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Calculate the number of features after the feature extractor\n",
    "        # ----------------------------\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input tensor with the correct input size\n",
    "            dummy_input = torch.zeros(1, 6, 24, 36)  # Batch size 1, 6 channels, 24x36 image\n",
    "            features_output = self.conv1(dummy_input)\n",
    "            n_features = features_output.shape[1] * features_output.shape[2] * features_output.shape[3]\n",
    "            \n",
    "        # Shared fully connected layers\n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_features, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # Separate output layers\n",
    "        self.fc_regression = nn.Linear(100, 1)      # For Niño3.4 regression output\n",
    "        self.fc_classification = nn.Linear(100, 5)  # For 5-class classification output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x_shared = self.fc_shared(x)\n",
    "        x_regression = self.fc_regression(x_shared)\n",
    "        x_classification = self.fc_classification(x_shared)\n",
    "        return x_regression, x_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model_class()\n",
    "X = torch.randn(1, 6, 24, 36)\n",
    "\n",
    "# Conv 部分\n",
    "for layer in net.conv1:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "\n",
    "# Flatten the features into a 1D tensor\n",
    "X = torch.flatten(X, 1)  # Flatten all dimensions except batch dimension\n",
    "    \n",
    "# Shared fully connected layers 部分\n",
    "for layer in net.fc_shared:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "    \n",
    "# Regression output for label_pr\n",
    "X_pr = net.fc_regression(X)\n",
    "print(f\"Regression output for label_pr形状:\\t{X_pr.shape}\")\n",
    "\n",
    "# Classification output for label_month\n",
    "X_month = net.fc_classification(X)\n",
    "print(f\"Regression output for label_pr形状:\\t{X_month.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images and labels from NetCDF files for multi-task learning.\n",
    "\n",
    "    Each sample consists of:\n",
    "    - image: ssta data at a given time index, shape (6, 24, 36)\n",
    "    - label_nino34: 'Niño3.4' variable at the same time index, scalar regression target\n",
    "    - label_softmax5: 'softmax5' variable at the same time index, integer from 0 to 4 (5 classes)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_root='.',   # Current working directory\n",
    "                 phase='train',   # 'train' or 'val' to specify dataset phase\n",
    "                 normalize=False,  # Whether to normalize the input data\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading data from NetCDF files and splitting into train and val sets.\n",
    "\n",
    "        Args:\n",
    "            data_root (str): Root directory containing the 'train' folder with the data files.\n",
    "            phase (str): Indicates whether to load 'train' or 'val' dataset.\n",
    "            normalize (bool): If True, normalize the input images.\n",
    "            transform (callable, optional): Optional transform to be applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load input and label data\n",
    "        # ----------------------------\n",
    "\n",
    "        # File paths for input data and labels\n",
    "        input_file = os.path.join(data_root, 'train', '')\n",
    "        label_file = os.path.join(data_root, 'train', '')\n",
    "\n",
    "        # Load the input data using xarray\n",
    "        self.input_ds = xr.open_dataset(input_file)\n",
    "        # Extract the 'ssta' variable which has dimensions (time, lev, latitude, longitude)\n",
    "        self.images = self.input_ds['ssta']\n",
    "\n",
    "        # Load the label data using xarray\n",
    "        self.label_ds = xr.open_dataset(label_file)\n",
    "        # Extract 'Niño3.4' and 'softmax5' variables\n",
    "        self.labels_nino34 = self.label_ds['Niño3.4']\n",
    "        self.labels_softmax5 = self.label_ds['softmax5']\n",
    "\n",
    "        # ----------------------------\n",
    "        # Ensure time alignment\n",
    "        # ----------------------------\n",
    "\n",
    "        # Number of samples\n",
    "        self.length = self.images.sizes['time']  # Should be 1490\n",
    "\n",
    "        # ----------------------------\n",
    "        # Split data into train and val sets\n",
    "        # ----------------------------\n",
    "\n",
    "        # Create an array of indices\n",
    "        indices = np.arange(self.length)\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(indices)\n",
    "        # Compute the split index\n",
    "        split_idx = int(0.7 * self.length)\n",
    "        # Split indices into train and val\n",
    "        train_indices = indices[:split_idx]\n",
    "        val_indices = indices[split_idx:]\n",
    "\n",
    "        # Assign indices based on phase\n",
    "        if phase == 'train':\n",
    "            self.indices = train_indices\n",
    "        elif phase == 'val':\n",
    "            self.indices = val_indices\n",
    "        else:\n",
    "            raise ValueError(\"phase must be 'train' or 'val'\")\n",
    "\n",
    "        # Store the transform if provided\n",
    "        self.transform = transform\n",
    "\n",
    "        # Store normalization flag\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Precompute mean and std if normalization is True\n",
    "        if self.normalize:\n",
    "            # For simplicity, we'll assume mean and std are 0 and 1 (no normalization)\n",
    "            self.mean = 0.0\n",
    "            self.std = 1.0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and labels at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            image (Tensor): The input image tensor of shape (6, 24, 36).\n",
    "            labels (tuple): A tuple containing:\n",
    "                - label_nino34 (Tensor): Scalar tensor, the 'Niño3.4' variable (regression target).\n",
    "                - label_softmax5 (Tensor): Scalar tensor, the 'softmax5' variable (classification target).\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the actual index in the dataset\n",
    "        actual_idx = self.indices[idx]\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the image\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the image data at the given time index\n",
    "        # Shape: (lev, latitude, longitude) = (6, 24, 36)\n",
    "        image = self.images.isel(time=actual_idx)\n",
    "\n",
    "        # Convert to NumPy array and ensure type float32\n",
    "        image = image.values.astype(np.float32)\n",
    "\n",
    "        # If normalization is True, apply standardization\n",
    "        if self.normalize:\n",
    "            image = (image - self.mean) / self.std\n",
    "\n",
    "        # If a transform is provided (e.g., additional preprocessing), apply it to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the 'Niño3.4' label\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the 'Niño3.4' label at the given time index\n",
    "        label_nino34 = self.labels_nino34.isel(time=actual_idx)\n",
    "\n",
    "        # Convert to NumPy scalar and ensure type float32\n",
    "        label_nino34 = label_nino34.values.astype(np.float32)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        label_nino34 = torch.tensor(label_nino34)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the 'softmax5' label\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the 'softmax5' label at the given time index\n",
    "        label_softmax5 = self.labels_softmax5.isel(time=actual_idx)\n",
    "\n",
    "        # Convert to integer\n",
    "        label_softmax5 = int(label_softmax5.values)\n",
    "\n",
    "        # Convert to PyTorch tensor of type long\n",
    "        label_softmax5 = torch.tensor(label_softmax5, dtype=torch.long)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Return the image and labels\n",
    "        # ----------------------------\n",
    "\n",
    "        # Return the image and a tuple of labels for multi-task learning\n",
    "        return image, (label_nino34, label_softmax5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for MyDataset\n",
    "# Create training and validation datasets\n",
    "train_dataset = MyDataset(data_root='.', phase='train', normalize=False)\n",
    "val_dataset = MyDataset(data_root='.', phase='val', normalize=False)\n",
    "\n",
    "# Access a sample from the training dataset\n",
    "image, (label_nino34, label_softmax5) = train_dataset[0]\n",
    "\n",
    "print(\"Image shape:\", image.shape)  # Should be (6, 24, 36)\n",
    "print(\"Niño3.4 label:\", label_nino34.item())\n",
    "print(\"Softmax5 label:\", label_softmax5.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=25, save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    训练多任务学习模型的函数。\n",
    "\n",
    "    参数：\n",
    "        model: 要训练的模型（Model_class）。\n",
    "        train_dataloader: 训练数据集的 DataLoader。\n",
    "        val_dataloader: 验证数据集的 DataLoader。\n",
    "        optimizer: 优化器。\n",
    "        scheduler: 学习率调度器。\n",
    "        num_epochs: 训练的轮数。\n",
    "        save_dir: 模型和结果保存的目录。\n",
    "    \"\"\"\n",
    "    # 设置设备（GPU 或 CPU）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 初始化最佳模型的权重和最佳验证损失\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # 保存训练和验证过程中的统计结果\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_mae_nino34\": [],\n",
    "        \"train_mse_nino34\": [],\n",
    "        \"train_r2_nino34\": [],\n",
    "        \"train_acc_softmax5\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_mae_nino34\": [],\n",
    "        \"val_mse_nino34\": [],\n",
    "        \"val_r2_nino34\": [],\n",
    "        \"val_acc_softmax5\": [],\n",
    "    }\n",
    "\n",
    "    # 确保保存目录存在\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 开始训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 30)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print('-' * 30)\n",
    "\n",
    "        # 每个 epoch 包含一个训练阶段和一个验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 设置模型为训练模式\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.eval()   # 设置模型为验证模式\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            # 初始化本阶段的指标\n",
    "            metrics = defaultdict(float)\n",
    "            # 样本总数\n",
    "            total_samples = 0\n",
    "            # 存储预测值和真实值，用于计算统计指标\n",
    "            preds_nino34_list = []\n",
    "            labels_nino34_list = []\n",
    "            preds_softmax5_list = []\n",
    "            labels_softmax5_list = []\n",
    "\n",
    "            # 使用 tqdm 显示进度条\n",
    "            bar = tqdm.tqdm(dataloader)\n",
    "            for inputs, (labels_nino34, labels_softmax5) in bar:\n",
    "                # 将数据移动到设备上\n",
    "                inputs = inputs.to(device)\n",
    "                labels_nino34 = labels_nino34.to(device)\n",
    "                labels_softmax5 = labels_softmax5.to(device)\n",
    "\n",
    "                # 优化器梯度清零\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向传播\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 获取模型输出\n",
    "                    outputs_nino34, outputs_softmax5 = model(inputs)\n",
    "\n",
    "                    # 计算损失\n",
    "                    loss_nino34 = nn.MSELoss()(outputs_nino34.squeeze(), labels_nino34)\n",
    "                    loss_softmax5 = nn.CrossEntropyLoss()(outputs_softmax5, labels_softmax5)\n",
    "                    loss = 0.8 * loss_softmax5 + 0.2 * loss_nino34  # 多任务的总损失\n",
    "\n",
    "                    # 获取预测值\n",
    "                    preds_nino34 = outputs_nino34.detach().squeeze()\n",
    "                    preds_softmax5 = torch.argmax(outputs_softmax5, dim=1)\n",
    "\n",
    "                    # 训练阶段进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 更新指标\n",
    "                batch_size = inputs.size(0)\n",
    "                metrics['loss'] += loss.item() * batch_size  # 累积损失\n",
    "\n",
    "                # 回归任务的平均绝对误差（MAE）和均方误差（MSE）\n",
    "                mae_nino34 = F.l1_loss(preds_nino34, labels_nino34, reduction='mean')\n",
    "                mse_nino34 = F.mse_loss(preds_nino34, labels_nino34, reduction='mean')\n",
    "                metrics['mae_nino34'] += mae_nino34.item() * batch_size\n",
    "                metrics['mse_nino34'] += mse_nino34.item() * batch_size\n",
    "\n",
    "                # 分类任务的正确预测数量\n",
    "                corrects_softmax5 = torch.sum(preds_softmax5 == labels_softmax5)\n",
    "                metrics['corrects_softmax5'] += corrects_softmax5.item()\n",
    "\n",
    "                # 样本总数\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # 存储预测值和真实值\n",
    "                preds_nino34_list.append(preds_nino34.cpu().numpy())\n",
    "                labels_nino34_list.append(labels_nino34.cpu().numpy())\n",
    "                preds_softmax5_list.append(preds_softmax5.cpu().numpy())\n",
    "                labels_softmax5_list.append(labels_softmax5.cpu().numpy())\n",
    "\n",
    "                # 更新进度条显示\n",
    "                bar.set_description(f\"{phase.capitalize()} Loss: {metrics['loss'] / total_samples :.4f}\")\n",
    "                \n",
    "                # Update progress bar with current metrics\n",
    "                bar.set_postfix({\n",
    "                    \"MSE_Nino34\": f\"{metrics['mse_nino34']/total_samples:.4f}\",\n",
    "                    \"Acc_Softmax5\": f\"{metrics['corrects_softmax5']/total_samples:.4f}\"\n",
    "                })\n",
    "                \n",
    "            # 计算本阶段的平均损失和指标\n",
    "            # R2越接近1，表示拟合效果越好\n",
    "            epoch_loss = metrics['loss'] / total_samples\n",
    "            epoch_mae_nino34 = metrics['mae_nino34'] / total_samples\n",
    "            epoch_mse_nino34 = metrics['mse_nino34'] / total_samples\n",
    "            epoch_r2_nino34 = r2_score(np.concatenate(labels_nino34_list, axis=0),\n",
    "                                       np.concatenate(preds_nino34_list, axis=0))\n",
    "            epoch_acc_softmax5 = metrics['corrects_softmax5'] / total_samples\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f}\\n\"\n",
    "                  f\"MSE_Nino34: {epoch_mse_nino34:.4f}\\n\"\n",
    "                  f\"MAE_Nino34: {epoch_mae_nino34:.4f}\\n\"\n",
    "                  f\"R2_Nino34: {epoch_r2_nino34:.4f}\\n Acc_Softmax5: {epoch_acc_softmax5:.4f}\\n\")\n",
    "\n",
    "            # 计算分类任务的其他指标\n",
    "            labels_softmax5_array = np.concatenate(labels_softmax5_list, axis=0)\n",
    "            preds_softmax5_array = np.concatenate(preds_softmax5_list, axis=0)\n",
    "            cm_softmax5 = confusion_matrix(labels_softmax5_array, preds_softmax5_array)\n",
    "            precision_softmax5 = precision_score(labels_softmax5_array, preds_softmax5_array, average='macro', zero_division=0)\n",
    "            recall_softmax5 = recall_score(labels_softmax5_array, preds_softmax5_array, average='macro')\n",
    "            f1_softmax5 = f1_score(labels_softmax5_array, preds_softmax5_array, average='macro')\n",
    "\n",
    "            print(f\"{phase.capitalize()} Softmax5 Classification Metrics:\")\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(cm_softmax5)\n",
    "            print(f\"Precision_Softmax5: {precision_softmax5 :.4f} Recall_Softmax5: {recall_softmax5 :.4f} F1-score_Softmax5: {f1_softmax5 :.4f}\")\n",
    "\n",
    "            # 保存结果\n",
    "            if phase == 'train':\n",
    "                results[\"train_loss\"].append(epoch_loss)\n",
    "                results[\"train_mae_nino34\"].append(epoch_mae_nino34)\n",
    "                results[\"train_mse_nino34\"].append(epoch_mse_nino34)\n",
    "                results[\"train_r2_nino34\"].append(epoch_r2_nino34)\n",
    "                results[\"train_acc_softmax5\"].append(epoch_acc_softmax5)\n",
    "            else:\n",
    "                results[\"val_loss\"].append(epoch_loss)\n",
    "                results[\"val_mae_nino34\"].append(epoch_mae_nino34)\n",
    "                results[\"val_mse_nino34\"].append(epoch_mse_nino34)\n",
    "                results[\"val_r2_nino34\"].append(epoch_r2_nino34)\n",
    "                results[\"val_acc_softmax5\"].append(epoch_acc_softmax5)\n",
    "\n",
    "                # 如果当前验证损失小于最佳验证损失，保存模型\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"Saving the best model based on Validation Loss.\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(best_model_wts, os.path.join(save_dir, \"best_model.pth\"))\n",
    "                    print(\"Best model saved\")\n",
    "\n",
    "                    # 保存最佳模型的指标到文件\n",
    "                    with open(os.path.join(save_dir, \"best_metrics.txt\"), \"w\") as f:\n",
    "                        f.write(f\"Epoch: {epoch + 1}\\n\")\n",
    "                        f.write(f\"Val Loss: {epoch_loss:.4f}\\n\")\n",
    "                        f.write(f\"Val MAE_Nino34: {epoch_mae_nino34:.4f}\\n\")\n",
    "                        f.write(f\"Val MSE_Nino34: {epoch_mse_nino34:.4f}\\n\")\n",
    "                        f.write(f\"Val R2_Nino34: {epoch_r2_nino34:.4f}\\n\")\n",
    "                        f.write(f\"Val Acc_Softmax5: {epoch_acc_softmax5:.4f}\\n\")\n",
    "                        f.write(f\"Precision: {precision_softmax5:.4f}\\n\")\n",
    "                        f.write(f\"Recall: {recall_softmax5:.4f}\\n\")\n",
    "                        f.write(f\"F1-score: {f1_softmax5:.4f}\\n\")\n",
    "                        f.write(\"Confusion Matrix:\\n\")\n",
    "                        f.write(f\"{cm_softmax5}\\n\")\n",
    "\n",
    "            # 每 10 个 epoch 保存一次模型\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"epoch_{epoch + 1}.pth\"))\n",
    "                print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "        # 学习率调度器更新\n",
    "        scheduler.step()\n",
    "\n",
    "        # 绘制并保存训练曲线\n",
    "        epochs = np.arange(1, epoch + 2)\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(epochs, results[\"val_loss\"], label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"loss_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_acc_softmax5\"], label=\"Train Acc Softmax5\")\n",
    "        plt.plot(epochs, results[\"val_acc_softmax5\"], label=\"Val Acc Softmax5\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy Curve (Softmax5 Classification)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"accuracy_curve_softmax5.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_mae_nino34\"], label=\"Train MAE Nino34\")\n",
    "        plt.plot(epochs, results[\"val_mae_nino34\"], label=\"Val MAE Nino34\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MAE\")\n",
    "        plt.title(\"MAE Curve (Nino34 Regression)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"mae_curve_nino34.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_r2_nino34\"], label=\"Train R2 Nino34\")\n",
    "        plt.plot(epochs, results[\"val_r2_nino34\"], label=\"Val R2 Nino34\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"R2 Score\")\n",
    "        plt.title(\"R2 Score Curve (Nino34 Regression)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"r2_curve_nino34.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 训练完成后，加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Training complete. Best validation loss: {:.4f}\".format(best_val_loss))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(1024)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model_class()\n",
    "# model = model.to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "# model.load_state_dict(torch.load('checkpoints/train_1/best_model.pth'))\n",
    "\n",
    "train_set = MyDataset(data_root=\".\", phase=\"train\", normalize=False)\n",
    "valid_set = MyDataset(data_root=\".\", phase=\"val\", normalize=False)\n",
    "train_dataloader = DataLoader(train_set, batch_size=30, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_dataloader = DataLoader(valid_set, batch_size=30, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=30, eta_min=1e-6)\n",
    "\n",
    "model = train_model(model, train_dataloader, val_dataloader, optimizer_ft, exp_lr_scheduler, num_epochs=25, save_dir=\"checkpoints/train_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
