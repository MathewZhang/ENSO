{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import copy\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, precision_score, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "# 绘制混淆矩阵图\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified AlexNet model for multi-task learning.\n",
    "    \n",
    "    This model outputs:\n",
    "    - Regression output of size 23 for label_pr.\n",
    "    - Classification output of size 12 for label_month.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # ----------------------------\n",
    "        # Feature extraction layers\n",
    "        # ----------------------------\n",
    "        self.features = nn.Sequential(\n",
    "            # Convolutional layer 1\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),  # Adjusted input channels to 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            # Convolutional layer 2\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            # Convolutional layer 3\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Convolutional layer 4\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Convolutional layer 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Adjusted pooling layer to prevent feature map from becoming too small\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),  # Modified kernel size and stride\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Calculate the number of features after the feature extractor\n",
    "        # ----------------------------\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input tensor with the correct input size\n",
    "            dummy_input = torch.zeros(1, 3, 24, 36)  # Batch size 1, 3 channels, 24x36 image\n",
    "            features_output = self.features(dummy_input)\n",
    "            n_features = features_output.shape[1] * features_output.shape[2] * features_output.shape[3]\n",
    "            # In this configuration, n_features should be 512 (256 * 1 * 2)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Shared fully connected layers\n",
    "        # ----------------------------\n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(n_features, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Output layers for each task\n",
    "        # ----------------------------\n",
    "        # Regression output for label_pr (size 23)\n",
    "        self.fc_pr = nn.Linear(4096, 23)  # Regression output of size 23\n",
    "\n",
    "        # Classification output for label_month (12 classes)\n",
    "        self.fc_month = nn.Linear(4096, 12)  # Classification output of size 12\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through feature extractor\n",
    "        x = self.features(x)\n",
    "        # Flatten the features into a 1D tensor\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch dimension\n",
    "\n",
    "        # Pass through shared fully connected layers\n",
    "        x = self.fc_shared(x)\n",
    "\n",
    "        # Pass through task-specific output layers\n",
    "        output_pr = self.fc_pr(x)        # Regression output for label_pr\n",
    "        output_month = self.fc_month(x)  # Classification output for label_month\n",
    "\n",
    "        return output_pr, output_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet()\n",
    "X = torch.randn(1, 3, 24, 36)\n",
    "\n",
    "# feature 部分\n",
    "for layer in net.features:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "\n",
    "# Flatten the features into a 1D tensor\n",
    "X = torch.flatten(X, 1)  # Flatten all dimensions except batch dimension\n",
    "    \n",
    "# Shared fully connected layers 部分\n",
    "for layer in net.fc_shared:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "    \n",
    "# Regression output for label_pr\n",
    "X_pr = net.fc_pr(X)\n",
    "print(f\"Regression output for label_pr形状:\\t{X_pr.shape}\")\n",
    "\n",
    "# Classification output for label_month\n",
    "X_month = net.fc_month(X)\n",
    "print(f\"Regression output for label_pr形状:\\t{X_month.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images and labels from NetCDF files for multi-task learning.\n",
    "\n",
    "    Each sample consists of:\n",
    "    - image: sst data at a given time index, shape (3, 24, 36)\n",
    "    - label_pr: 'pr' variable at the same time index, shape (23,), regression target\n",
    "    - label_month: 'month_label' variable at the same time index, integer from 0 to 11 (12 classes)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_root='.',   # Current working directory\n",
    "                 phase='train',    # 'train' or 'val' to specify dataset phase\n",
    "                 normalize=False,   # Whether to normalize the input data\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading data from NetCDF files.\n",
    "\n",
    "        Args:\n",
    "            data_root (str): Root directory containing the data folders 'train' and 'val'.\n",
    "            phase (str): Indicates whether to load 'train' or 'val' dataset.\n",
    "            normalize (bool): If True, normalize the input images.\n",
    "            transform (callable, optional): Optional transform to be applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Set dataset paths based on phase\n",
    "        # ----------------------------\n",
    "        if phase == 'train':\n",
    "            input_file = os.path.join(data_root, 'train', '')\n",
    "            label_file = os.path.join(data_root, 'train', '')\n",
    "        elif phase == 'val':\n",
    "            input_file = os.path.join(data_root, 'val', '')\n",
    "            label_file = os.path.join(data_root, 'val', '')\n",
    "        else:\n",
    "            raise ValueError(\"phase must be 'train' or 'val'\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load image data\n",
    "        # ----------------------------\n",
    "        # Load the image data using xarray\n",
    "        self.image_ds = xr.open_dataset(input_file)\n",
    "        # Extract the 'sst' variable which has dimensions (time, lev, lat, lon)\n",
    "        self.images = self.image_ds['sst']\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load label data\n",
    "        # ----------------------------\n",
    "        # Load the label data using xarray\n",
    "        self.label_ds = xr.open_dataset(label_file)\n",
    "        # Extract the 'pr' variable (time, lev, lat, lon) and 'month_label' (time, lat, lon)\n",
    "        self.labels_pr = self.label_ds['pr']\n",
    "        self.labels_month = self.label_ds['month_label']\n",
    "\n",
    "        # Get the number of samples from the time dimension\n",
    "        self.length = self.images.sizes['time']\n",
    "\n",
    "        # Store the transform if provided\n",
    "        self.transform = transform\n",
    "\n",
    "        # Store normalization flag\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Precompute mean and std if normalization is True\n",
    "        if self.normalize:\n",
    "            # Compute mean and std over the dataset\n",
    "            # Note: Depending on the size of the dataset, this might be memory-intensive\n",
    "            # Alternative: Use predefined mean and std values if known\n",
    "            # Here, we will compute mean and std over the 'lev' (channel), 'lat', and 'lon' dimensions\n",
    "            # For simplicity, we'll assume mean and std are 0 and 1 (i.e., standardization is not applied)\n",
    "            # Users can compute and set their own mean and std if needed\n",
    "            # self.mean = self.images.mean(dim=('time', 'lat', 'lon')).values\n",
    "            # self.std = self.images.std(dim=('time', 'lat', 'lon')).values\n",
    "            # For this example, we set mean and std to 0 and 1\n",
    "            self.mean = 0.0\n",
    "            self.std = 1.0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and labels at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            image (Tensor): The input image tensor of shape (3, 24, 36).\n",
    "            labels (tuple): A tuple containing:\n",
    "                - label_pr (Tensor): Tensor of shape (23,), the 'pr' variable (regression target).\n",
    "                - label_month (Tensor): Scalar tensor, the month label (0-11) for classification.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the image\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the image data at the given time index\n",
    "        # Resulting shape: (lev, lat, lon) = (3, 24, 36)\n",
    "        image = self.images.isel(time=idx)\n",
    "\n",
    "        # Convert the xarray DataArray to a NumPy array and ensure it's of type float32\n",
    "        image = image.values.astype(np.float32)\n",
    "\n",
    "        # If normalization is True, apply standardization\n",
    "        if self.normalize:\n",
    "            image = (image - self.mean) / self.std\n",
    "\n",
    "        # If a transform is provided (e.g., additional preprocessing), apply it to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert the NumPy array to a PyTorch tensor\n",
    "        # Final shape: (3, 24, 36)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the 'pr' label\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the 'pr' label at the given time index\n",
    "        # Initial shape: (lev, lat, lon) = (23, 1, 1)\n",
    "        label_pr = self.labels_pr.isel(time=idx)\n",
    "\n",
    "        # Squeeze singleton dimensions (lat and lon) to get shape (23,)\n",
    "        label_pr = label_pr.values.squeeze()\n",
    "\n",
    "        # Ensure the label is of type float32\n",
    "        label_pr = label_pr.astype(np.float32)\n",
    "\n",
    "        # Convert to a PyTorch tensor\n",
    "        label_pr = torch.from_numpy(label_pr)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load and process the 'month_label'\n",
    "        # ----------------------------\n",
    "\n",
    "        # Select the 'month_label' at the given time index\n",
    "        # Initial shape: (lat, lon) = (1, 1)\n",
    "        label_month = self.labels_month.isel(time=idx)\n",
    "\n",
    "        # Squeeze singleton dimensions to get a scalar value\n",
    "        label_month = label_month.values.squeeze()\n",
    "\n",
    "        # Adjust month label to be in range 0-11 for classification (if needed)\n",
    "        # Assuming label_month is in 1-12\n",
    "        label_month = int(label_month) - 1  # Adjust to 0-11\n",
    "\n",
    "        # Convert to a PyTorch tensor of type long (integer type)\n",
    "        label_month = torch.tensor(label_month, dtype=torch.long)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Return the image and labels\n",
    "        # ----------------------------\n",
    "\n",
    "        # Return the image and a tuple of labels for multi-task learning\n",
    "        return image, (label_pr, label_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=25, save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    训练多任务学习模型的函数。\n",
    "\n",
    "    参数：\n",
    "        model: 要训练的模型（AlexNet）。\n",
    "        train_dataloader: 训练数据集的 DataLoader。\n",
    "        val_dataloader: 验证数据集的 DataLoader。\n",
    "        optimizer: 优化器。\n",
    "        scheduler: 学习率调度器。\n",
    "        num_epochs: 训练的轮数。\n",
    "        save_dir: 模型和结果保存的目录。\n",
    "    \"\"\"\n",
    "    # 设置设备（GPU 或 CPU）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 初始化最佳模型的权重和最佳验证损失\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # 保存训练和验证过程中的统计结果\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_mae_pr\": [],\n",
    "        \"train_mse_pr\": [],\n",
    "        \"train_r2_pr\": [],\n",
    "        \"train_acc_month\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_mae_pr\": [],\n",
    "        \"val_mse_pr\": [],\n",
    "        \"val_r2_pr\": [],\n",
    "        \"val_acc_month\": [],\n",
    "    }\n",
    "\n",
    "    # 确保保存目录存在\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 开始训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 30)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print('-' * 30)\n",
    "\n",
    "        # 每个 epoch 包含一个训练阶段和一个验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 设置模型为训练模式\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.eval()   # 设置模型为验证模式\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            # 初始化本阶段的指标\n",
    "            metrics = defaultdict(float)\n",
    "            # 样本总数\n",
    "            total_samples = 0\n",
    "            # 存储预测值和真实值，用于计算统计指标\n",
    "            preds_pr_list = []\n",
    "            labels_pr_list = []\n",
    "            preds_month_list = []\n",
    "            labels_month_list = []\n",
    "\n",
    "            # 使用 tqdm 显示进度条\n",
    "            bar = tqdm.tqdm(dataloader)\n",
    "            for inputs, (labels_pr, labels_month) in bar:\n",
    "                # 将数据移动到设备上\n",
    "                inputs = inputs.to(device)\n",
    "                labels_pr = labels_pr.to(device)\n",
    "                labels_month = labels_month.to(device)\n",
    "\n",
    "                # 优化器梯度清零\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向传播\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 获取模型输出\n",
    "                    outputs_pr, outputs_month = model(inputs)\n",
    "\n",
    "                    # 计算损失\n",
    "                    loss_pr = nn.MSELoss()(outputs_pr, labels_pr)  # 回归任务的均方误差损失\n",
    "                    loss_month = nn.CrossEntropyLoss()(outputs_month, labels_month)  # 分类任务的交叉熵损失\n",
    "                    loss = 0.8 * loss_pr + 0.2 * loss_month  # 多任务的总损失\n",
    "\n",
    "                    # 获取预测值\n",
    "                    preds_pr = outputs_pr.detach()# 获取值，并保证不参与梯度计算\n",
    "                    preds_month = torch.argmax(outputs_month, 1)# 1表示在第二个维度中选择；0表示第一个维度，batch siz\n",
    "\n",
    "                    # 训练阶段进行反向传播和优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 更新指标\n",
    "                batch_size = inputs.size(0)\n",
    "                metrics['loss'] += loss.item() * batch_size  # 累积损失\n",
    "\n",
    "                # 回归任务的平均绝对误差（MAE）和均方误差（MSE），经过累加后，metrics里存的值都需要再除以总数\n",
    "                mae_pr = F.l1_loss(preds_pr, labels_pr, reduction='mean')\n",
    "                mse_pr = F.mse_loss(preds_pr, labels_pr, reduction='mean')\n",
    "                metrics['mae_pr'] += mae_pr.item() * batch_size\n",
    "                metrics['mse_pr'] += mse_pr.item() * batch_size\n",
    "\n",
    "                # 分类任务的正确预测数量\n",
    "                corrects_month = torch.sum(preds_month == labels_month)\n",
    "                metrics['corrects_month'] += corrects_month.item()\n",
    "\n",
    "                # 样本总数\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # 存储预测值和真实值\n",
    "                preds_pr_list.append(preds_pr.cpu().numpy())\n",
    "                labels_pr_list.append(labels_pr.cpu().numpy())\n",
    "                preds_month_list.append(preds_month.cpu().numpy())\n",
    "                labels_month_list.append(labels_month.cpu().numpy())\n",
    "\n",
    "                # 更新进度条显示\n",
    "                bar.set_description(f\"{phase.capitalize()} Loss: {metrics['loss'] / total_samples :.4f}\")\n",
    "                \n",
    "                # Update progress bar with current metrics\n",
    "                bar.set_postfix({\n",
    "                    \"MSE_PR\": f\"{metrics['mse_pr']/total_samples:.4f}\",\n",
    "                    \"Acc_Month\": f\"{metrics['corrects_month']/total_samples:.4f}\"\n",
    "                })\n",
    "                \n",
    "            # 计算本阶段的平均损失和指标\n",
    "            # R2越接近1，表示拟合效果越好\n",
    "            epoch_loss = metrics['loss'] / total_samples\n",
    "            epoch_mae_pr = metrics['mae_pr'] / total_samples\n",
    "            epoch_mse_pr = metrics['mse_pr'] / total_samples\n",
    "            epoch_r2_pr = r2_score(np.concatenate(labels_pr_list, axis=0),\n",
    "                                   np.concatenate(preds_pr_list, axis=0))\n",
    "            epoch_acc_month = metrics['corrects_month'] / total_samples\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f}\\n\"\n",
    "                  f\"MSE_PR: {epoch_mse_pr:.4f}\\n\"\n",
    "                  f\"MAE_PR: {epoch_mae_pr:.4f}\\n\"\n",
    "                  f\"R2_PR: {epoch_r2_pr:.4f}\\n Acc_Month: {epoch_acc_month:.4f}\\n\")\n",
    "\n",
    "            # 计算分类任务的其他指标\n",
    "            labels_month_array = np.concatenate(labels_month_list, axis=0)\n",
    "            preds_month_array = np.concatenate(preds_month_list, axis=0)\n",
    "            cm_month = confusion_matrix(labels_month_array, preds_month_array)\n",
    "            precision_month = precision_score(labels_month_array, preds_month_array, average='macro', zero_division=0)\n",
    "            recall_month = recall_score(labels_month_array, preds_month_array, average='macro')\n",
    "            f1_month = f1_score(labels_month_array, preds_month_array, average='macro')\n",
    "\n",
    "            print(f\"{phase.capitalize()} Month Classification Metrics:\")\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(cm_month)\n",
    "            print(f\"Precision_month: {precision_month :.4f} Recall_month: {recall_month :.4f} F1-score_month: {f1_month :.4f}\")\n",
    "\n",
    "            # Plot and save the confusion matrix heatmap\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "            # sns.heatmap(cm_month, annot=True, fmt='d', cmap='Blues')\n",
    "            # plt.xlabel('Predicted')\n",
    "            # plt.ylabel('True')\n",
    "            # plt.title(f'{phase.capitalize()} Confusion Matrix (Epoch {epoch + 1})')\n",
    "            # plt.savefig(os.path.join(save_dir, f\"{phase}_confusion_matrix_epoch_{epoch + 1}.png\"))\n",
    "            # plt.close()\n",
    "            \n",
    "            # 保存结果\n",
    "            if phase == 'train':\n",
    "                results[\"train_loss\"].append(epoch_loss)\n",
    "                results[\"train_mae_pr\"].append(epoch_mae_pr)\n",
    "                results[\"train_mse_pr\"].append(epoch_mse_pr)\n",
    "                results[\"train_r2_pr\"].append(epoch_r2_pr)\n",
    "                results[\"train_acc_month\"].append(epoch_acc_month)\n",
    "            else:\n",
    "                results[\"val_loss\"].append(epoch_loss)\n",
    "                results[\"val_mae_pr\"].append(epoch_mae_pr)\n",
    "                results[\"val_mse_pr\"].append(epoch_mse_pr)\n",
    "                results[\"val_r2_pr\"].append(epoch_r2_pr)\n",
    "                results[\"val_acc_month\"].append(epoch_acc_month)\n",
    "\n",
    "                # 如果当前验证损失小于最佳验证损失，保存模型\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"Saving the best model based on Validation Accuracy for label_month.\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(best_model_wts, os.path.join(save_dir, \"best_model.pth\"))\n",
    "                    print(\"Best model saved\")\n",
    "\n",
    "                    # 保存最佳模型的指标到文件\n",
    "                    with open(os.path.join(save_dir, \"best_metrics.txt\"), \"w\") as f:\n",
    "                        f.write(f\"Epoch: {epoch + 1}\\n\")\n",
    "                        f.write(f\"Val Loss: {epoch_loss:.4f}\\n\")\n",
    "                        f.write(f\"Val MAE_PR: {epoch_mae_pr:.4f}\\n\")\n",
    "                        f.write(f\"Val MSE_PR: {epoch_mse_pr:.4f}\\n\")\n",
    "                        f.write(f\"Val R2_PR: {epoch_r2_pr:.4f}\\n\")\n",
    "                        f.write(f\"Val Acc_Month: {epoch_acc_month:.4f}\\n\")\n",
    "                        f.write(f\"Precision: {precision_month:.4f}\\n\")\n",
    "                        f.write(f\"Recall: {recall_month:.4f}\\n\")\n",
    "                        f.write(f\"F1-score: {f1_month:.4f}\\n\")\n",
    "                        f.write(\"Confusion Matrix:\\n\")\n",
    "                        f.write(f\"{cm_month}\\n\")\n",
    "\n",
    "            # 每 10 个 epoch 保存一次模型\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"epoch_{epoch + 1}.pth\"))\n",
    "                print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "        # 学习率调度器更新\n",
    "        scheduler.step()\n",
    "\n",
    "        # 绘制并保存训练曲线\n",
    "        epochs = np.arange(1, epoch + 2)\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(epochs, results[\"val_loss\"], label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"loss_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_acc_month\"], label=\"Train Acc Month\")\n",
    "        plt.plot(epochs, results[\"val_acc_month\"], label=\"Val Acc Month\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy Curve (Month Classification)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"accuracy_curve_month.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_mae_pr\"], label=\"Train MAE PR\")\n",
    "        plt.plot(epochs, results[\"val_mae_pr\"], label=\"Val MAE PR\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MAE\")\n",
    "        plt.title(\"MAE Curve (PR Regression)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"mae_curve_pr.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, results[\"train_r2_pr\"], label=\"Train R2 PR\")\n",
    "        plt.plot(epochs, results[\"val_r2_pr\"], label=\"Val R2 PR\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"R2 Score\")\n",
    "        plt.title(\"R2 Score Curve (PR Regression)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"r2_curve_pr.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 训练完成后，加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Training complete. Best validation loss: {:.4f}\".format(best_val_loss))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个train方法not used\n",
    "def train2_model(model, \n",
    "               train_dataloader, \n",
    "               val_dataloader, \n",
    "               optimizer, \n",
    "               scheduler, \n",
    "               num_epochs=25, \n",
    "               save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Trains and validates the given model using the provided dataloaders, optimizer, and scheduler.\n",
    "    Implements multi-task learning with two outputs: regression for label_pr and classification for label_month.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The CNN model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        val_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 25.\n",
    "        save_dir (str, optional): Directory to save model checkpoints and metrics. Defaults to \"checkpoints\".\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The best model based on validation accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize variables to track the best model and best validation accuracy\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc_month = 0  # Best validation accuracy for label_month classification\n",
    "    best_val_loss_pr = float('inf')  # Best validation loss for label_pr regression\n",
    "\n",
    "    # Initialize a dictionary to store training and validation metrics\n",
    "    results = {\n",
    "        \"train_loss_pr\": [],\n",
    "        \"train_loss_month\": [],\n",
    "        \"train_loss_total\": [],\n",
    "        \"train_rmse_pr\": [],\n",
    "        \"train_acc_month\": [],\n",
    "        \"val_loss_pr\": [],\n",
    "        \"val_loss_month\": [],\n",
    "        \"val_loss_total\": [],\n",
    "        \"val_rmse_pr\": [],\n",
    "        \"val_acc_month\": []\n",
    "    }\n",
    "\n",
    "    # Move the model to the appropriate device (GPU if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss functions for each task\n",
    "    criterion_pr = nn.MSELoss()  # Mean Squared Error for regression task\n",
    "    criterion_month = nn.CrossEntropyLoss()  # Cross-Entropy Loss for classification task\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Training Phase\n",
    "        # ----------------------------\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Initialize metrics for training\n",
    "        train_metrics = defaultdict(float)\n",
    "        train_metrics['loss_pr'] = 0.0\n",
    "        train_metrics['loss_month'] = 0.0\n",
    "        train_metrics['loss_total'] = 0.0\n",
    "        train_metrics['rmse_pr'] = 0.0\n",
    "        train_metrics['acc_month'] = 0.0\n",
    "        train_samples = 0  # Number of samples processed\n",
    "\n",
    "        # Create a tqdm progress bar for the training loop\n",
    "        train_bar = tqdm.tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "        for images, (labels_pr, labels_month) in train_bar:\n",
    "            # Move data to the appropriate device\n",
    "            images = images.to(device)  # Input images\n",
    "            labels_pr = labels_pr.to(device)  # Regression labels\n",
    "            labels_month = labels_month.to(device)  # Classification labels\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute model outputs\n",
    "            outputs_pr, outputs_month = model(images)\n",
    "\n",
    "            # Compute individual losses\n",
    "            loss_pr = criterion_pr(outputs_pr, labels_pr)  # Regression loss\n",
    "            loss_month = criterion_month(outputs_month, labels_month)  # Classification loss\n",
    "\n",
    "            # Combine losses with specified weights for multi-task learning\n",
    "            loss = 0.8 * loss_pr + 0.2 * loss_month\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute metrics\n",
    "            with torch.no_grad():\n",
    "                # Calculate RMSE for regression\n",
    "                rmse_pr = torch.sqrt(loss_pr).item()\n",
    "\n",
    "                # Calculate accuracy for classification\n",
    "                _, preds_month = torch.max(outputs_month, 1)\n",
    "                acc_month = torch.sum(preds_month == labels_month).item() / labels_month.size(0)\n",
    "\n",
    "            # Update training metrics\n",
    "            train_metrics['loss_pr'] += loss_pr.item() * images.size(0)\n",
    "            train_metrics['loss_month'] += loss_month.item() * images.size(0)\n",
    "            train_metrics['loss_total'] += loss.item() * images.size(0)\n",
    "            train_metrics['rmse_pr'] += rmse_pr * images.size(0)\n",
    "            train_metrics['acc_month'] += acc_month * images.size(0)\n",
    "            train_samples += images.size(0)\n",
    "\n",
    "            # Update progress bar with current metrics\n",
    "            train_bar.set_postfix({\n",
    "                \"Loss_PR\": f\"{train_metrics['loss_pr']/train_samples:.4f}\",\n",
    "                \"Loss_Month\": f\"{train_metrics['loss_month']/train_samples:.4f}\",\n",
    "                \"RMSE_PR\": f\"{train_metrics['rmse_pr']/train_samples:.4f}\",\n",
    "                \"Acc_Month\": f\"{train_metrics['acc_month']/train_samples:.4f}\"\n",
    "            })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        epoch_train_loss_pr = train_metrics['loss_pr'] / train_samples\n",
    "        epoch_train_loss_month = train_metrics['loss_month'] / train_samples\n",
    "        epoch_train_loss_total = train_metrics['loss_total'] / train_samples\n",
    "        epoch_train_rmse_pr = train_metrics['rmse_pr'] / train_samples\n",
    "        epoch_train_acc_month = train_metrics['acc_month'] / train_samples\n",
    "\n",
    "        # Store training metrics\n",
    "        results[\"train_loss_pr\"].append(epoch_train_loss_pr)\n",
    "        results[\"train_loss_month\"].append(epoch_train_loss_month)\n",
    "        results[\"train_loss_total\"].append(epoch_train_loss_total)\n",
    "        results[\"train_rmse_pr\"].append(epoch_train_rmse_pr)\n",
    "        results[\"train_acc_month\"].append(epoch_train_acc_month)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Validation Phase\n",
    "        # ----------------------------\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        # Initialize metrics for validation\n",
    "        val_metrics = defaultdict(float)\n",
    "        val_metrics['loss_pr'] = 0.0\n",
    "        val_metrics['loss_month'] = 0.0\n",
    "        val_metrics['loss_total'] = 0.0\n",
    "        val_metrics['rmse_pr'] = 0.0\n",
    "        val_metrics['acc_month'] = 0.0\n",
    "        val_samples = 0  # Number of samples processed\n",
    "\n",
    "        # Lists to store true and predicted labels for computing confusion matrix and other metrics\n",
    "        true_month = []\n",
    "        pred_month = []\n",
    "\n",
    "        # Create a tqdm progress bar for the validation loop\n",
    "        val_bar = tqdm.tqdm(val_dataloader, desc=\"Validation\", leave=False)\n",
    "        for images, (labels_pr, labels_month) in val_bar:\n",
    "            # Move data to the appropriate device\n",
    "            images = images.to(device)  # Input images\n",
    "            labels_pr = labels_pr.to(device)  # Regression labels\n",
    "            labels_month = labels_month.to(device)  # Classification labels\n",
    "\n",
    "            # Forward pass: compute model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs_pr, outputs_month = model(images)\n",
    "\n",
    "                # Compute individual losses\n",
    "                loss_pr = criterion_pr(outputs_pr, labels_pr)  # Regression loss\n",
    "                loss_month = criterion_month(outputs_month, labels_month)  # Classification loss\n",
    "\n",
    "                # Combine losses with specified weights for multi-task learning\n",
    "                loss = 0.8 * loss_pr + 0.2 * loss_month\n",
    "\n",
    "                # Compute metrics\n",
    "                rmse_pr = torch.sqrt(loss_pr).item()\n",
    "                _, preds_month_batch = torch.max(outputs_month, 1)\n",
    "                acc_month = torch.sum(preds_month_batch == labels_month).item() / labels_month.size(0)\n",
    "\n",
    "            # Update validation metrics\n",
    "            val_metrics['loss_pr'] += loss_pr.item() * images.size(0)\n",
    "            val_metrics['loss_month'] += loss_month.item() * images.size(0)\n",
    "            val_metrics['loss_total'] += loss.item() * images.size(0)\n",
    "            val_metrics['rmse_pr'] += rmse_pr * images.size(0)\n",
    "            val_metrics['acc_month'] += acc_month * images.size(0)\n",
    "            val_samples += images.size(0)\n",
    "\n",
    "            # Collect true and predicted labels for classification metrics\n",
    "            true_month.extend(labels_month.cpu().numpy())\n",
    "            pred_month.extend(preds_month_batch.cpu().numpy())\n",
    "\n",
    "            # Update progress bar with current metrics\n",
    "            val_bar.set_postfix({\n",
    "                \"Loss_PR\": f\"{val_metrics['loss_pr']/val_samples:.4f}\",\n",
    "                \"Loss_Month\": f\"{val_metrics['loss_month']/val_samples:.4f}\",\n",
    "                \"RMSE_PR\": f\"{val_metrics['rmse_pr']/val_samples:.4f}\",\n",
    "                \"Acc_Month\": f\"{val_metrics['acc_month']/val_samples:.4f}\"\n",
    "            })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        epoch_val_loss_pr = val_metrics['loss_pr'] / val_samples\n",
    "        epoch_val_loss_month = val_metrics['loss_month'] / val_samples\n",
    "        epoch_val_loss_total = val_metrics['loss_total'] / val_samples\n",
    "        epoch_val_rmse_pr = val_metrics['rmse_pr'] / val_samples\n",
    "        epoch_val_acc_month = val_metrics['acc_month'] / val_samples\n",
    "\n",
    "        # Store validation metrics\n",
    "        results[\"val_loss_pr\"].append(epoch_val_loss_pr)\n",
    "        results[\"val_loss_month\"].append(epoch_val_loss_month)\n",
    "        results[\"val_loss_total\"].append(epoch_val_loss_total)\n",
    "        results[\"val_rmse_pr\"].append(epoch_val_rmse_pr)\n",
    "        results[\"val_acc_month\"].append(epoch_val_acc_month)\n",
    "\n",
    "        # Calculate additional classification metrics for label_month\n",
    "        cm_month = confusion_matrix(true_month, pred_month)\n",
    "        precision_month = precision_score(true_month, pred_month, average=\"macro\")\n",
    "        recall_month = recall_score(true_month, pred_month, average=\"macro\")\n",
    "        f1_month = f1_score(true_month, pred_month, average=\"macro\")\n",
    "\n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Loss PR: {epoch_val_loss_pr:.4f}, Loss Month: {epoch_val_loss_month:.4f}, Total Loss: {epoch_val_loss_total:.4f}\")\n",
    "        print(f\"Validation RMSE PR: {epoch_val_rmse_pr:.4f}\")\n",
    "        print(f\"Validation Accuracy Month: {epoch_val_acc_month:.4f}\")\n",
    "        print(\"Confusion Matrix for label_month:\")\n",
    "        print(cm_month)\n",
    "        print(f\"Precision Month: {precision_month:.4f}, Recall Month: {recall_month:.4f}, F1 Score Month: {f1_month:.4f}\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # ----------------------------\n",
    "        # Save the Best Model\n",
    "        # ----------------------------\n",
    "        # Define a criterion to determine the best model\n",
    "        # Here, we use validation accuracy for label_month\n",
    "        if epoch_val_acc_month > best_val_acc_month:\n",
    "            print(\"Saving the best model based on Validation Accuracy for label_month.\")\n",
    "            best_val_acc_month = epoch_val_acc_month\n",
    "            best_val_loss_pr = epoch_val_loss_pr\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, os.path.join(save_dir, \"best_model.pth\"))\n",
    "\n",
    "            # Save confusion matrix and other metrics to a text file\n",
    "            with open(os.path.join(save_dir, \"metrics_best.txt\"), \"w\") as f:\n",
    "                f.write(f\"Epoch: {epoch+1}\\n\")\n",
    "                f.write(f\"Validation Loss PR: {epoch_val_loss_pr:.4f}\\n\")\n",
    "                f.write(f\"Validation Loss Month: {epoch_val_loss_month:.4f}\\n\")\n",
    "                f.write(f\"Validation RMSE PR: {epoch_val_rmse_pr:.4f}\\n\")\n",
    "                f.write(f\"Validation Accuracy Month: {epoch_val_acc_month:.4f}\\n\")\n",
    "                f.write(\"Confusion Matrix for label_month:\\n\")\n",
    "                f.write(np.array2string(cm_month))\n",
    "                f.write(f\"\\nPrecision Month: {precision_month:.4f}\\n\")\n",
    "                f.write(f\"Recall Month: {recall_month:.4f}\\n\")\n",
    "                f.write(f\"F1 Score Month: {f1_month:.4f}\\n\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # Save Model Checkpoints Every 10 Epochs\n",
    "        # ----------------------------\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # Plot and Save Training Progress\n",
    "        # ----------------------------\n",
    "        # Plot Accuracy\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, epoch+2), results[\"train_acc_month\"], label=\"Train Accuracy Month\")\n",
    "        plt.plot(range(1, epoch+2), results[\"val_acc_month\"], label=\"Validation Accuracy Month\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Training and Validation Accuracy for label_month\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_dir, \"accuracy.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Loss for label_pr\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, epoch+2), results[\"train_loss_pr\"], label=\"Train Loss PR\")\n",
    "        plt.plot(range(1, epoch+2), results[\"val_loss_pr\"], label=\"Validation Loss PR\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss for label_pr\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_dir, \"loss_pr.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Loss for label_month\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, epoch+2), results[\"train_loss_month\"], label=\"Train Loss Month\")\n",
    "        plt.plot(range(1, epoch+2), results[\"val_loss_month\"], label=\"Validation Loss Month\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss for label_month\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_dir, \"loss_month.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"\\nTraining complete. Best Validation Accuracy Month: {best_val_acc_month:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(1024)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AlexNet()\n",
    "# model = model.to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load('checkpoints/train_1/best_model.pth'))\n",
    "\n",
    "train_set = MyDataset(data_root=\".\", phase=\"train\", normalize=False)\n",
    "valid_set = MyDataset(data_root=\".\", phase=\"val\", normalize=False)\n",
    "train_dataloader = DataLoader(train_set, batch_size=30, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_dataloader = DataLoader(valid_set, batch_size=30, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=30, eta_min=1e-6)\n",
    "\n",
    "model = train_model(model, train_dataloader, val_dataloader, optimizer_ft, exp_lr_scheduler, num_epochs=30, save_dir=\"checkpoints/train_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
